{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plot\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (77697, 6000)\n",
      "y_train shape:  (77697,)\n",
      "X_test shape:  (19425, 6000)\n",
      "y_test shape:  (19425,)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "with open('tfidfvec.pickle', 'rb') as handle:\n",
    "    X_train, X_test, y_train, y_test = pickle.load(handle)\n",
    "    \n",
    "# print shapes\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\n",
    "\n",
    "Naive bayes classifier does not allow for negative values in the document vectors, therefore, we can't use word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------TRAINING METRICS---------\n",
      "accuracy:  0.6943382627385871\n",
      "\n",
      "precision:  [0.68057543 0.70989144]\n",
      "recall:  [0.72611036 0.66291186]\n",
      "f1 score:  [0.70260591 0.68559779]\n",
      "auc:  0.6945111096034594\n",
      "[[28054 10582]\n",
      " [13167 25894]]\n",
      "\n",
      "---------TEST METRICS---------\n",
      "accuracy:  0.6721750321750322\n",
      "\n",
      "precision:  [0.65658622 0.68983311]\n",
      "recall:  [0.70569851 0.63942601]\n",
      "f1 score:  [0.68025708 0.66367381]\n",
      "auc:  0.6725622614405332\n",
      "[[6774 2825]\n",
      " [3543 6283]]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model = MultinomialNB();\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# ------------------------\n",
    "# TRAINING metrics\n",
    "# ------------------------\n",
    "\n",
    "print('---------TRAINING METRICS---------')\n",
    "\n",
    "# accuracy\n",
    "score = model.score(X_train, y_train)\n",
    "print('accuracy: ', score); print()\n",
    "\n",
    "# precision, recall\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_train, y_pred_train)\n",
    "print('precision: ', precision)\n",
    "print('recall: ', recall)\n",
    "print('f1 score: ', f1)\n",
    "\n",
    "# auc\n",
    "auc = roc_auc_score(y_train, y_pred_train)\n",
    "print('auc: ', auc)\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_train, y_pred_train)\n",
    "print(cm); print()\n",
    "\n",
    "# -----------------------\n",
    "# TEST metrics\n",
    "# -----------------------\n",
    "\n",
    "print('---------TEST METRICS---------')\n",
    "\n",
    "# accuracy\n",
    "score = model.score(X_test, y_test)\n",
    "print('accuracy: ', score); print()\n",
    "\n",
    "# precision, recall\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred_test)\n",
    "print('precision: ', precision)\n",
    "print('recall: ', recall)\n",
    "print('f1 score: ', f1)\n",
    "\n",
    "# auc\n",
    "auc = roc_auc_score(y_test, y_pred_test)\n",
    "print('auc: ', auc)\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count Vectorization**\n",
    "\n",
    "training score:  0.6964\n",
    "\n",
    "precision:  [0.68699839 0.70672149]  \n",
    "recall:  [0.72001923 0.67284493]  \n",
    "f1 score:  [0.70312133 0.68936728]  \n",
    "\n",
    "confusion matrix:  \n",
    "([[17976,  6990],  \n",
    "  [ 8190, 16844]])\n",
    "  \n",
    "  \n",
    "**TFIDF Vectorization**\n",
    "\n",
    "training score:  0.70352\n",
    "\n",
    "precision:  [0.69444018 0.71341973]  \n",
    "recall:  [0.72542658 0.68167292]  \n",
    "f1 score:  [0.70959527 0.69718511]  \n",
    "\n",
    "confusion matrix:  \n",
    "([[18111,  6855]  \n",
    "  [ 7969, 17065]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
